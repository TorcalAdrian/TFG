{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreacion del codigo para ejecutar el algoritmo txmeans, puediendolo depurar de una manera mas eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'C:\\Users\\adria\\Desktop\\TFG\\TXMeans\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from generators.datamanager import *\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros a probar\n",
    "vector_size = 100\n",
    "window = 10\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../dataset/mushrooms.csv\"\n",
    "\n",
    "df = pd.read_csv(datapath)\n",
    "missing_symbol='?'\n",
    "index_mode = dict()\n",
    "\n",
    "for k, index in zip(df.columns, range(len(df.columns))):\n",
    "    df[k] = df[k].replace(missing_symbol, np.nan)\n",
    "    mode_value = df[k].mode()[0]\n",
    "    df[k] = df[k].fillna(mode_value)\n",
    "    index_mode[index] = mode_value\n",
    "\n",
    "columns_to_use = df.columns[df.columns != 'class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df[columns_to_use].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['processed_text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=df['processed_text'], vector_size=vector_size, window=window, min_count=1, workers=4, sg=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_average_vector(words, model):\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(normalize(model.wv[word].reshape(1, -1))[0])\n",
    "    if vectors:\n",
    "        return normalize(np.mean(vectors, axis=0).reshape(1, -1))[0]\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "X = np.array([get_average_vector(text, model) for text in df['processed_text']])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤══════════╤══════════╤══════════╤════════════════╕\n",
      "│   vector_size │   window │   epochs │      NMI │ running_time   │\n",
      "╞═══════════════╪══════════╪══════════╪══════════╪════════════════╡\n",
      "│           100 │       10 │       50 │ 0.238949 │ 0:00:00.204200 │\n",
      "╘═══════════════╧══════════╧══════════╧══════════╧════════════════╛\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 2\n",
    "start_time = datetime.datetime.now()\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X)\n",
    "end_time = datetime.datetime.now()\n",
    "running_time = end_time - start_time\n",
    "\n",
    "\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "label_map = {'p': 0, 'e': 1}\n",
    "real_labels_numeric = df['class'].map(label_map)\n",
    "\n",
    "nmi = normalized_mutual_info_score(real_labels_numeric, y_kmeans)\n",
    "\n",
    "results.append({\n",
    "    \"vector_size\": vector_size, \n",
    "    \"window\": window, \n",
    "    \"epochs\": epochs, \n",
    "    \"NMI\": nmi, \n",
    "    \"running_time\": running_time\n",
    "})\n",
    "print(tabulate(results, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
